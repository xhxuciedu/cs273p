{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feedforward_neural_net.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xhxuciedu/cs273p/blob/main/feedforward_neural_net_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a simple example of a feedforward neural network implemented using PyTorch. This network is designed for a generic classification task, where the goal is to predict the class of an input vector. The network consists of an input layer, a few hidden layers, and an output layer. The exact number of layers and their sizes can be adjusted based on the specific requirements of your task."
      ],
      "metadata": {
        "id": "sFcNGlLobLZN"
      }
    },
    {
      "metadata": {
        "id": "JWSDGcM8fPzE"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GE--hQPqs_2_"
      },
      "cell_type": "markdown",
      "source": [
        "### Check GPU availability"
      ]
    },
    {
      "metadata": {
        "id": "Zt89xrC4sHt0"
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nY6V-tL7fWTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a5fc96-7971-427b-b3d3-8931b6c8aa7d"
      },
      "cell_type": "code",
      "source": [
        "device"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a feedforward neural net model"
      ],
      "metadata": {
        "id": "iFCHe-BPa5Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedforwardNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FeedforwardNeuralNet, self).__init__()\n",
        "        # First hidden layer\n",
        "        self.hidden1 = nn.Linear(input_size, hidden_size)\n",
        "        # Second hidden layer\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
        "        # Output layer\n",
        "        self.output = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the first hidden layer with ReLU activation\n",
        "        x = F.relu(self.hidden1(x))\n",
        "        # Forward pass through the second hidden layer with ReLU activation\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        # Forward pass through the output layer\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "POtVCrhoajYo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xmGhHF2ws7W5"
      },
      "cell_type": "markdown",
      "source": [
        "### Hypter-parameters"
      ]
    },
    {
      "metadata": {
        "id": "ljCaUcrMs0G-"
      },
      "cell_type": "code",
      "source": [
        "# model hyper-parameters\n",
        "input_size = 784  # For MNIST dataset, for example\n",
        "hidden_size = 500 # Number of hidden neurons\n",
        "num_classes = 10  # Number of output classes (e.g., MNIST has 10 digits)\n",
        "\n",
        "# training hyper-parameters\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the neural network\n",
        "model = FeedforwardNeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz_DIU_5bDkN",
        "outputId": "f1da71cb-37e0-4867-e7ed-5f308a9585e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedforwardNeuralNet(\n",
            "  (hidden1): Linear(in_features=784, out_features=500, bias=True)\n",
            "  (hidden2): Linear(in_features=500, out_features=500, bias=True)\n",
            "  (output): Linear(in_features=500, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the code**\n",
        "\n",
        "This code defines a neural network (`FeedforwardNeuralNet`) with two hidden layers, each followed by a ReLU activation function.\n",
        "\n",
        "The `__init__` method initializes the layers of the network, and the `forward` method defines how the data flows through the network, i.e., it specifies the forward pass.\n",
        "\n",
        "- `input_size`: The size of the input features (e.g., for the MNIST dataset, this would be 28x28=784, assuming you flatten the images into a 784-dimensional vector).\n",
        "- `hidden_size`: The number of neurons in each hidden layer. You can adjust this based on the complexity of your task.\n",
        "- `num_classes`: The number of output classes for your classification task."
      ],
      "metadata": {
        "id": "dm9spj5Fb5hM"
      }
    },
    {
      "metadata": {
        "id": "3-NnJw6Cs2B7"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ]
    },
    {
      "metadata": {
        "id": "vnrhtrekfNHn"
      },
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cnYLrSQ0sdo7"
      },
      "cell_type": "markdown",
      "source": [
        "### Check the data and labels"
      ]
    },
    {
      "metadata": {
        "id": "AnMfEressM9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1357d5c-2184-44a3-8993-b76e861b3b62"
      },
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "print(images.shape, labels.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "b-S6w8tysho6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "85fbba0e-ff7d-4fe6-9c7f-28118f61df9f"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(images[1,0,:,:])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e7f7cc4efd0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "58yoIgrvssPs"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "qQgNgTnwdUH0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sR0rH5GFfJbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e55f9a-411d-48ae-86e1-334733978327"
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.train\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.0262\n",
            "Epoch [1/5], Step [200/600], Loss: 0.0171\n",
            "Epoch [1/5], Step [300/600], Loss: 0.0291\n",
            "Epoch [1/5], Step [400/600], Loss: 0.0123\n",
            "Epoch [1/5], Step [500/600], Loss: 0.0882\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0238\n",
            "Epoch [2/5], Step [100/600], Loss: 0.1208\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0723\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0645\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0130\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0046\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0165\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0127\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0306\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0634\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0300\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0312\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0310\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0385\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0013\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0123\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0317\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0444\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0021\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0305\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0006\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0018\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0005\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0057\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0030\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ufEUHOYAsvKw"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, device):\n",
        "    \"\"\"Evaluates the performance of a trained model on the test dataset.\"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            images = images.view(images.size(0), -1)  # Flatten images if needed\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the class with highest probability\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the network on the {total} test images: {accuracy:.2f} %')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "0zUBEh-YeDjT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLvHHB1qerkq",
        "outputId": "15e21a5d-6c82-4506-8605-15f62123fe3e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 98.00 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "g8f5kfBjfGDL"
      },
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pCYpDfGyfydT"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the code\n",
        "\n",
        "This script does the following:\n",
        "\n",
        "- Generates a synthetic dataset with 1000 samples where each sample has 784 features (like flattened images) and belongs to one of 10 classes.\n",
        "- Defines a `DataLoader` for batch processing, which is essential for efficient training over large datasets.\n",
        "- Sets up a training loop that iterates over the dataset in mini-batches, performs the forward pass to compute the output and loss, performs the backward pass to compute gradients, and updates the model parameters.\n",
        "\n",
        "Replace the synthetic dataset with your actual data. If you're using a well-known dataset like MNIST, PyTorch's `torchvision` library provides convenient methods to download and load these datasets in a ready-to-use format.\n",
        "\n",
        "Remember, the actual performance and the necessity of tuning hyperparameters such as the learning rate, the size of the hidden layers, and the batch size depend on the specific characteristics of your dataset and the complexity of the problem you're trying to solve."
      ],
      "metadata": {
        "id": "tkdUXmoCdszs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WF1LcUT1dt9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}